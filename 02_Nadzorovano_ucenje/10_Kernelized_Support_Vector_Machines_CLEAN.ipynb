{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a9c1b8",
   "metadata": {},
   "source": [
    "# Kernelized Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237cf40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6835ca",
   "metadata": {},
   "source": [
    "## Linear models and nonlinear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74964b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plot_helpers import discrete_scatter\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(centers=4, random_state=8)\n",
    "\n",
    "y = y % 2\n",
    "\n",
    "discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc642545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from helpers.plot_2d_separator import plot_2d_separator\n",
    "\n",
    "linear_svm = LinearSVC(max_iter=10000).fit(X, y)\n",
    "\n",
    "plot_2d_separator(linear_svm, X)\n",
    "discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75833fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
    "from helpers import plot_helpers\n",
    "\n",
    "# add the squared second feature\n",
    "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
    "figure = plt.figure()\n",
    "\n",
    "# visualize in 3D\n",
    "ax = Axes3D(figure, elev=-152, azim=-26, auto_add_to_figure=False)\n",
    "figure.add_axes(ax)\n",
    "\n",
    "# plot first all the points with y == 0, then all with y == 1\n",
    "mask = y == 0\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b', s=60)\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^', s=60)\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm_3d = LinearSVC(max_iter=10000).fit(X_new, y)\n",
    "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
    "\n",
    "# show linear decision boundary\n",
    "figure = plt.figure()\n",
    "ax = Axes3D(figure, elev=-152, azim=-26, auto_add_to_figure=False)\n",
    "figure.add_axes(ax)\n",
    "\n",
    "xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n",
    "yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "\n",
    "ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b', s=60)\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^', s=60)\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83658407",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ = YY ** 2\n",
    "dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n",
    "plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],cmap=plot_helpers.cm2, alpha=0.5)\n",
    "discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2f875",
   "metadata": {},
   "source": [
    "## The kernel trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0584fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bab39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516c7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1734dda9",
   "metadata": {},
   "source": [
    "## Understanding SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b53ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import tools\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = tools.make_handcrafted_dataset()\n",
    "\n",
    "svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\n",
    "\n",
    "plot_2d_separator(svm, X, eps=.5)\n",
    "discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "# plot support vectors\n",
    "sv = svm.support_vectors_\n",
    "# class labels of support vectors are given by the sign of the dual coefficients\n",
    "sv_labels = svm.dual_coef_.ravel() > 0\n",
    "\n",
    "discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72915c9",
   "metadata": {},
   "source": [
    "## Tuning SVM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86475a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.8)\n",
    "\n",
    "\n",
    "def plot_svc(C=10, gamma=0.1):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "    model = SVC(kernel='rbf', C=C, gamma=gamma).fit(X, y)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, ax)\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, lw=1, facecolors='none');\n",
    "    ax.set_title(f'C = {C}, gamma = {gamma}', size=14)\n",
    "\n",
    "\n",
    "def plot_svc_interact():\n",
    "    return interact(plot_svc, C=[0.01, 0.1, 0.5, 1, 5, 10, 50], gamma=[0.001, 0.01, 0.1, 1, 10])\n",
    "\n",
    "plot_svc_interact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643464fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plot_rbf_svm_parameters import plot_svm\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for ax, C in zip(axes, [-1, 0, 3]):\n",
    "    for a, gamma in zip(ax, range(-1, 2)):\n",
    "        plot_svm(log_C=C, log_gamma=gamma, ax=a)\n",
    "\n",
    "axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"], ncol=4, loc=(.9, 1.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df980eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svc = SVC(gamma=\"auto\")\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy on training set: {svc.score(X_train, y_train):.2f}\")\n",
    "print(f\"Accuracy on test set: {svc.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train.min(axis=0), 'o', label=\"min\")\n",
    "plt.plot(X_train.max(axis=0), '^', label=\"max\")\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Feature magnitude\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f88cf4e",
   "metadata": {},
   "source": [
    "## Preprocessing data for SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427caf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53abb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(gamma=\"auto\")\n",
    "svc.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a64e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy on training set: {svc.score(X_train_scaled, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {svc.score(X_test_scaled, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1accd265",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1000, gamma=\"auto\")\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Accuracy on training set: {svc.score(X_train_scaled, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {svc.score(X_test_scaled, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939e08b5",
   "metadata": {},
   "source": [
    "## Strengths, weaknesses, and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d501a2",
   "metadata": {},
   "source": [
    "**Strengths:**\n",
    "- Kernelized support vector machines are powerful models and perform well on a variety of datasets. \n",
    "- SVMs allow for complex decision boundaries, even if the data has only a few features. \n",
    "- They work well on low-dimensional and high-dimensional data (i.e.,few and many features), but \n",
    "- Still, it might be worth trying SVMs, particularly if all of your features represent measurements in similar units (e.g., all are pixel intensities) and they are on similar scales.\n",
    "\n",
    "\n",
    "**Weaknesses:**\n",
    "- Donâ€™t scale very well with the number of samples. \n",
    "    - Running an SVM on data with up to 10,000 samples might work well, but working with datasets of size 100,000 or more can become challenging in terms of runtime and memory usage.\n",
    "- They require careful preprocessing of the data and tuning of the parameters.\n",
    "    - This is why, these days, most people instead use tree-based models such as random forests or gradient boosting (which require little or no preprocessing) in many applications.\n",
    "- SVM models are hard to inspect\n",
    "    - It can be difficult to understand why a particular prediction was made, and it might be tricky to explain the model to a nonexpert.\n",
    "\n",
    "**Parameters:**\n",
    "- the regularization parameter C\n",
    "- gamma\n",
    "        \n",
    "- gamma and C both control the complexity of the model, with large values in either resulting in a more complex model. \n",
    "- Therefore, good settings for the two parameters are usually strongly correlated, and C and gamma should be adjusted together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c2260-c9b7-4ca3-a67a-10b8ae31a3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
