{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29da7bbe",
   "metadata": {},
   "source": [
    "# Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6854d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6854bde",
   "metadata": {},
   "source": [
    "## What is Feature Scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17192a",
   "metadata": {},
   "source": [
    "**Feature scaling is a method used to normalize the range of independent variables or features of data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae228afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c9e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a45a1a36",
   "metadata": {},
   "source": [
    "## When to scale your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4aac8",
   "metadata": {},
   "source": [
    "- **`Gradient Descent Based Algorithms`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f8c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9e850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37cdb55f",
   "metadata": {},
   "source": [
    "<img width=\"500\" src=\"images/scaling_fig01.webp\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae821f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the Sklearn diabetes data set\n",
    "diabetes_X, diabetes_y = load_diabetes(return_X_y=True)\n",
    "\n",
    "# Create scaled data set\n",
    "raw = diabetes_X[:, None, 2]\n",
    "max_raw = max(raw)\n",
    "min_raw = min(raw)\n",
    "scaled = (2*raw - max_raw - min_raw)/(max_raw - min_raw)\n",
    "\n",
    "def train_raw_data():\n",
    "    LinearRegression().fit(raw, diabetes_y)\n",
    "\n",
    "def train_scaled_data():\n",
    "    LinearRegression().fit(scaled, diabetes_y)\n",
    "\n",
    "# Use the timeit method to measure the execution of training method\n",
    "raw_time = timeit.timeit(train_raw_data, number=1000)\n",
    "scaled_time = timeit.timeit(train_scaled_data, number=1000)\n",
    "\n",
    "# Print the time taken to train the model with raw data and scaled data\n",
    "print(f\"Raw data: {raw_time}s\")\n",
    "print(f\"Scaled data: {scaled_time}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daaccac",
   "metadata": {},
   "source": [
    "- **`Distance-Based Algorithms (very sensitive to the relative magnitudes of the different features)`**\n",
    "    - Distance-based algorithms like KNN, K-means, and SVM are most affected by the range of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02e06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bde70b4",
   "metadata": {},
   "source": [
    "- **`For feature engineering using PCA`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5ec00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd6043a",
   "metadata": {},
   "source": [
    "- **`Linear regression`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec690e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecdb0d3d",
   "metadata": {},
   "source": [
    "- **`L1 or L2 regularization`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1226176",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd2fae2d",
   "metadata": {},
   "source": [
    "## When scaling your data is not necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867df004",
   "metadata": {},
   "source": [
    "- **`Tree-based algorithms`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b2ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b49053e",
   "metadata": {},
   "source": [
    "## Different types of features scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2646a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import plot_scaling\n",
    "\n",
    "plot_scaling.plot_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe0834",
   "metadata": {},
   "source": [
    "- Use `MinMaxScaler` as your default\n",
    "- Use `RobustScaler` if you have outliers and can handle a larger range\n",
    "- Use `StandardScaler` if you need normalized features\n",
    "- Use `Normalizer` sparingly - it normalizes rows, not columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'WEIGHT': [15, 18, 12,10],\n",
    "                   'PRICE': [1,3,2,5]},\n",
    "                   index = ['Orange','Apple','Banana','Grape'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd6c62d",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f031cbb",
   "metadata": {},
   "source": [
    "- Also known as **min-max scaling** or **min-max normalization**, it is the simplest method and consists of **rescaling the range of features to scale the range in [0, 1]**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035bfc3",
   "metadata": {},
   "source": [
    "The general formula for normalization is given as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c5234",
   "metadata": {},
   "source": [
    "<img width=\"150\" src=\"images/formula_img01.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1950f9",
   "metadata": {},
   "source": [
    "> We can also do a normalization over different intervals, e.g. choosing to have the variable laying in any [a, b] interval, a and b being real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24779618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d870cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34619dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4462f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e25c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default range \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df1 = pd.DataFrame(scaler.fit_transform(df),\n",
    "                   columns=['WEIGHT','PRICE'],\n",
    "                   index = ['Orange','Apple','Banana','Grape'])\n",
    "\n",
    "ax = df.plot.scatter(x='WEIGHT', y='PRICE',color=['red','green','blue','yellow'], \n",
    "                     marker = '*',s=80, label='BREFORE SCALING');\n",
    "\n",
    "df1.plot.scatter(x='WEIGHT', y='PRICE', color=['red','green','blue','yellow'],\n",
    "                 marker = 'o',s=60,label='AFTER SCALING', ax = ax);\n",
    "plt.axhline(0, color='red',alpha=0.2)\n",
    "plt.axvline(0, color='red',alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [-1,1] range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "df1 = pd.DataFrame(scaler.fit_transform(df),\n",
    "                   columns=['WEIGHT','PRICE'],\n",
    "                   index = ['Orange','Apple','Banana','Grape'])\n",
    "\n",
    "ax = df.plot.scatter(x='WEIGHT', y='PRICE',color=['red','green','blue','yellow'], \n",
    "                     marker = '*',s=80, label='BREFORE SCALING');\n",
    "\n",
    "df1.plot.scatter(x='WEIGHT', y='PRICE', color=['red','green','blue','yellow'],\n",
    "                 marker = 'o',s=60,label='AFTER SCALING', ax = ax);\n",
    "plt.axhline(0, color='red',alpha=0.2)\n",
    "plt.axvline(0, color='red',alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85478a71",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50380280",
   "metadata": {},
   "source": [
    "- **Z-score normalization**, also known as **Z-score standardization** or **mean-variance scaling**.\n",
    "- Feature standardization makes the values of each feature in the data **have zero mean and a standard deviation of one (unit variance)**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9e5b0",
   "metadata": {},
   "source": [
    "The general method of calculation is to determine the distribution mean and standard deviation for each feature and calculate the new data point by the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272db92b",
   "metadata": {},
   "source": [
    "<img width=\"150\" src=\"images/formula_img02.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db2ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bb148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620eb070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df2 = pd.DataFrame(scaler.fit_transform(df),\n",
    "                   columns=['WEIGHT','PRICE'],\n",
    "                   index = ['Orange','Apple','Banana','Grape'])\n",
    "\n",
    "ax = df.plot.scatter(x='WEIGHT', y='PRICE',color=['red','green','blue','yellow'], \n",
    "                     marker = '*',s=80, label='BREFORE SCALING');\n",
    "df2.plot.scatter(x='WEIGHT', y='PRICE', color=['red','green','blue','yellow'],\n",
    "                 marker = 'o',s=60,label='AFTER SCALING', ax = ax)\n",
    "\n",
    "plt.axhline(0, color='red',alpha=0.2)\n",
    "plt.axvline(0, color='red',alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcbd78",
   "metadata": {},
   "source": [
    "Practical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa62bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Perceptron Model without Feature Scaling\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# features are sepal length and petal length\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "prcptrn = Perceptron(eta0=0.1, random_state=1)\n",
    "prcptrn.fit(X_train, y_train)\n",
    "\n",
    "y_predict = prcptrn.predict(X_test)\n",
    "print(f\"Misclassified examples {(y_test != y_predict).sum()}\")\n",
    "print(f\"Accuracy Score {accuracy_score(y_test, y_predict):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Perceptron Model with Feature Scaling\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# features are sepal length and petal length\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_scaled = sc.transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "prcptrn = Perceptron(eta0=0.1, random_state=1)\n",
    "prcptrn.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_predict = prcptrn.predict(X_test_scaled)\n",
    "print(f\"Misclassified examples {(y_test != y_predict).sum()}\")\n",
    "print(f\"Accuracy Score {accuracy_score(y_test, y_predict):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd3be7",
   "metadata": {},
   "source": [
    "### Robust Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096edc9",
   "metadata": {},
   "source": [
    "- Robust scaling is one of the **best scaling techniques when we have outliers present in our dataset**. It scales the data accordingly to the interquartile range (IQR = 75 Quartile â€” 25 Quartile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abae5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1cd677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e61032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "df3 = pd.DataFrame(scaler.fit_transform(df),\n",
    "                   columns=['WEIGHT','PRICE'],\n",
    "                   index = ['Orange','Apple','Banana','Grape'])\n",
    "\n",
    "ax = df.plot.scatter(x='WEIGHT', y='PRICE',color=['red','green','blue','yellow'], \n",
    "                     marker = '*',s=80, label='BREFORE SCALING');\n",
    "\n",
    "df3.plot.scatter(x='WEIGHT', y='PRICE', color=['red','green','blue','yellow'],\n",
    "                 marker = 'o',s=60,label='AFTER SCALING', ax = ax)\n",
    "plt.axhline(0, color='red',alpha=0.2)\n",
    "plt.axvline(0, color='red',alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fa28d",
   "metadata": {},
   "source": [
    "Effect of scaling using Standard Scaler and Robust Scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "dfr = pd.DataFrame({'WEIGHT': [15, 18, 12,10,50],\n",
    "                   'PRICE': [1,3,2,5,20]},\n",
    "                   index = ['Orange','Apple','Banana','Grape','Jackfruit'])\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df21 = pd.DataFrame(scaler.fit_transform(dfr),\n",
    "                   columns=['WEIGHT','PRICE'],\n",
    "                   index = ['Orange','Apple','Banana','Grape','Jackfruit'])\n",
    "\n",
    "ax = dfr.plot.scatter(x='WEIGHT', y='PRICE',color=['red','green','blue','yellow','black'], \n",
    "                     marker = '*',s=80, label='BREFORE SCALING')\n",
    "\n",
    "df21.plot.scatter(x='WEIGHT', y='PRICE', color=['red','green','blue','yellow','black'],\n",
    "                 marker = 'o',s=60,label='STANDARD', ax = ax,figsize=(12,6))\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "df31 = pd.DataFrame(scaler.fit_transform(dfr),\n",
    "                   columns=['WEIGHT','PRICE'],\n",
    "                   index = ['Orange','Apple','Banana','Grape','Jackfruit'])\n",
    "\n",
    "df31.plot.scatter(x='WEIGHT', y='PRICE', color=['red','green','blue','yellow','black'],\n",
    "                 marker = 'v',s=60,label='ROBUST', ax = ax,figsize=(12,6))\n",
    "\n",
    "plt.axhline(0, color='red',alpha=0.2)\n",
    "plt.axvline(0, color='red',alpha=0.2);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5247f295",
   "metadata": {},
   "source": [
    "### Scaling to unit length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303dcd24",
   "metadata": {},
   "source": [
    "**Normalizer works on the rows, not the columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977a2568",
   "metadata": {},
   "source": [
    "- It scales each data point such that the **feature vector has a Euclidean length of 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d1d740",
   "metadata": {},
   "source": [
    "This usually means dividing each component by the Euclidean length of the vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac2e316",
   "metadata": {},
   "source": [
    "<img width=\"350\" src=\"images/formula_img03.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32415b7",
   "metadata": {},
   "source": [
    "## Applying Scaling Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc221ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea41652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ad014",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614cb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset properties before and after scaling\n",
    "print(f\"transformed shape: {X_train_scaled.shape}\")\n",
    "print(f\"per-feature minimum before scaling:\\n {X_train.min(axis=0)}\")\n",
    "print(f\"per-feature maximum before scaling:\\n {X_train.max(axis=0)}\")\n",
    "print(f\"per-feature minimum after scaling:\\n {X_train_scaled.min(axis=0)}\")\n",
    "print(f\"per-feature maximum after scaling:\\n {X_train_scaled.max(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# print test data properties after scaling\n",
    "print(f\"per-feature minimum after scaling:\\n{X_test_scaled.min(axis=0)}\")\n",
    "print(f\"per-feature maximum after scaling:\\n{X_test_scaled.max(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8492273",
   "metadata": {},
   "source": [
    "**MinMaxScaler (and all the other scalers) always\n",
    "applies exactly the same transformation to the training and the test set**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5841278b",
   "metadata": {},
   "source": [
    "## Scaling Training and Test Data the Same Way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbacb06",
   "metadata": {},
   "source": [
    "**It is important to apply exactly the same transformation to the training set and the test set for the supervised model to work on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6fc7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141af824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make synthetic data\n",
    "X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n",
    "\n",
    "# split it into training and test sets\n",
    "X_train, X_test = train_test_split(X, random_state=5, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8518142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and test sets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1], c=\"blue\", label=\"Training set\", s=60)\n",
    "axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^', c=\"red\", label=\"Test set\", s=60)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].set_title(\"Original Data\")\n",
    "\n",
    "# scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# visualize the properly scaled data\n",
    "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=\"blue\", label=\"Training set\", s=60)\n",
    "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',c=\"red\", label=\"Test set\", s=60)\n",
    "axes[1].set_title(\"Scaled Data\")\n",
    "\n",
    "# rescale the test set separately\n",
    "# so test set min is 0 and test set max is 1\n",
    "# DO NOT DO THIS! For illustration purposes only.\n",
    "test_scaler = MinMaxScaler()\n",
    "test_scaler.fit(X_test)\n",
    "X_test_scaled_badly = test_scaler.transform(X_test)\n",
    "\n",
    "# visualize wrongly scaled data\n",
    "axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=\"blue\", label=\"training set\", s=60)\n",
    "axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1], marker='^', c=\"red\", label=\"test set\", s=60)\n",
    "axes[2].set_title(\"Improperly Scaled Data\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b44697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# calling fit and transform in sequence (using method chaining)\n",
    "X_scaled = scaler.fit(X).transform(X)\n",
    "\n",
    "# same result, but more efficient computation\n",
    "X_scaled_d = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0661971",
   "metadata": {},
   "source": [
    "## The Effect of Preprocessing on Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Test set accuracy: {svm.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# scoring on the scaled test set\n",
    "print(f\"Scaled test set accuracy: {svm.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0027d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing using zero mean and unit variance scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# scoring on the scaled test set\n",
    "print(f\"Test set accuracy: {svm.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# scoring on the scaled test set\n",
    "print(f\"Test set accuracy: {svm.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8968d4b",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201f7bc",
   "metadata": {},
   "source": [
    "<img src=\"images/scaling_fig02.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
